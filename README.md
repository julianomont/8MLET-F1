# Books API

**Tech Challenge - Fase 1 - Welcome to Machine Learning Engineering**

* **Por:** Juliano Monteiro (rm369594)
* **GitHub:** [https://github.com/julianomont](https://github.com/julianomont)
* **Email:** [juliano.monteiro@outlook.com](mailto:juliano.monteiro@outlook.com)
* **LinkedIn:** [https://www.linkedin.com/in/julianofmonteiro](https://www.linkedin.com/in/julianofmonteiro)

API REST para consulta de dados de livros extra√≠dos via web scraping do site [books.toscrape.com](http://books.toscrape.com).

### Links Importantes

| Recurso | Link |
|---------|------|
| **Documenta√ß√£o Swagger** | <a href="https://books-api-o2d6gk76rq-uc.a.run.app/docs" target="_blank">Ver Documenta√ß√£o</a> |
| **Dashboard** | <a href="https://books-dashboard-o2d6gk76rq-uc.a.run.app" target="_blank">Acessar Dashboard</a> |
| **Deploy** | <a href="https://books-api-o2d6gk76rq-uc.a.run.app" target="_blank">API Base URL</a> |
| **Reposit√≥rio** | <a href="https://github.com/julianomont/8MLET-F1.git" target="_blank">GitHub</a> |
| **V√≠deo Demonstra√ß√£o** | em breve |

---

## √çndice

1. [Resumo T√©cnico: Books API (Tech Challenge Fase 1: Welcome to Machine Learning Engineering)](#resumo-tecnico-books-api-tech-challenge-fase-1-welcome-to-machine-learning-engineering)
2. [Requisitos do Projeto](#descri√ß√£o-do-projeto)
3. [Arquitetura](#arquitetura)
4. [Hospedagem no Google Cloud Platform (GCP)](#hospedagem-no-google-cloud-platform-gcp)
5. [Instala√ß√£o e Configura√ß√£o](#instala√ß√£o-e-configura√ß√£o)
6. [Execu√ß√£o](#execu√ß√£o)
7. [Documenta√ß√£o das Rotas da API](#documenta√ß√£o-das-rotas-da-api)
8. [Exemplos de Chamadas](#exemplos-de-chamadas)
9. [Dashboard de Monitoramento](#dashboard-de-monitoramento)

---

## Resumo T√©cnico: Books API (Tech Challenge Fase 1: Welcome to Machine Learning Engineering)

### Vis√£o Geral do Projeto

O projeto Books API foi desenvolvido como parte do Tech Challenge da p√≥s-gradua√ß√£o em Machine Learning Engineering (8MLET) da FIAP. O objetivo principal foi criar uma arquitetura robusta de Engenharia de Dados capaz de coletar, armazenar, processar e disponibilizar dados para futuros modelos de Machine Learning.

O sistema simula um pipeline real de dados, onde informa√ß√µes desestruturadas (HTML de um site de livros) s√£o transformadas em dados estruturados, enriquecidos e servidos via API para consumidores (Dashboards, Cientistas de Dados ou aplica√ß√µes front-end).

### Arquitetura da Solu√ß√£o
A solu√ß√£o adotou uma Clean Architecture modular, separando responsabilidades em camadas distintas para garantir manutenibilidade e escalabilidade:

- **Camada de Coleta (Ingest√£o)**: Respons√°vel pelo Web Scraping. Utiliza t√©cnicas de navega√ß√£o ass√≠ncrona para extrair dados de ~1000 livros do site Books to Scrape.
- **Camada de Persist√™ncia**: Utiliza um banco de dados relacional (PostgreSQL) hospedado na nuvem (Supabase/GCP) para garantir integridade e persist√™ncia dos dados.
- **Camada de API (Serving)**: Uma API RESTful desenvolvida em FastAPI, que exp√µe os dados coletados atrav√©s de endpoints documentados e protegidos.
- **Camada de Apresenta√ß√£o**: Um Dashboard interativo em Streamlit, focado em monitoramento de m√©tricas t√©cnicas e de neg√≥cio.

### Stack Tecnol√≥gica
As tecnologias foram escolhidas visando performance e modernidade:

| **Componente** | **Tecnologia** | **Justificativa** |
| --- | --- | --- |
| **Linguagem** | Python 3.11+ | Padr√£o da ind√∫stria para Data Science e ML. |
| **API Framework** | FastAPI | Alta performance (ass√≠ncrono), valida√ß√£o autom√°tica (Pydantic) e documenta√ß√£o nativa (Swagger). |
| **Banco de Dados** | PostgreSQL | SGBD relacional robusto, ideal para dados estruturados de livros e vendas. |
| **ORM** | SQLAlchemy 2.0 | Abstra√ß√£o segura do banco de dados com suporte a opera√ß√µes ass√≠ncronas. |
| **Scraping** | BeautifulSoup4 + httpx | BeautifulSoup para parse de HTML e httpx para requisi√ß√µes HTTP ass√≠ncronas concorrentes. |
| **Observabilidade** | Custom Middleware | Log estruturado e coleta de m√©tricas em tempo real (tempo de resposta, erros). |
| **Dashboard** | Streamlit | Cria√ß√£o r√°pida de interfaces de dados com Python puro. |
| **Infraestrutura** | Google Cloud Run | Deploy serverless (containers), focando em custo x benef√≠cio e escalabilidade zero-to-one. |

### Fluxo de Dados (Pipeline)

1. **Trigger**: O processo inicia via script (run_scraper.py) ou endpoint protegido (POST /scraping).
2. **Extra√ß√£o**: O Scraper navega nas p√°ginas de categorias do site alvo.
3. **Transforma√ß√£o**: Os dados brutos (pre√ßo com s√≠mbolos, strings de rating) s√£o limpos e convertidos para tipos num√©ricos/booleanos.
4. **Carga**: Os dados tratados s√£o salvos no PostgreSQL, atualizando registros existentes ou criando novos (Upsert logic).

**Consumo**:
- **API**: Disponibiliza queries complexas (filtros de pre√ßo, busca textual).
- **ML**: Endpoint /ml/training-data entrega o dataset pronto para treino (split features/target).
- **Dashboard**: Consome /metrics para monitorar a sa√∫de do sistema.

### Seguran√ßa e Infraestrutura
- **Autentica√ß√£o**: Implementada via JWT (JSON Web Tokens). Apenas usu√°rios autenticados (Admin) podem disparar cargas de dados ou acessar m√©tricas sens√≠veis.
- **Deploy (CI/CD)**:
  - Imagens Docker otimizadas (Multi-stage build).
  - Pipeline automatizado via Google Cloud Build.
  - Armazenamento de imagens no Artifact Registry.
  - Execu√ß√£o no Cloud Run com inje√ß√£o segura de segredos (DATABASE_URL, JWT_SECRET_KEY) em tempo de execu√ß√£o.

### Diferenciais do Projeto
- **Pipeline "ML-Ready"**: O sistema n√£o apenas armazena dados, mas j√° possui endpoints dedicados a servir Features pr√©-processadas para modelos de Machine Learning.
- **Monitoramento Integrado**: O dashboard n√£o √© apenas visual, ele consome m√©tricas reais da API, permitindo observar lat√™ncia e taxas de erro em tempo real.
- **Design Resiliente**: Tratamento de erros no scraper com retentativas autom√°ticas (retry logic) e fallback de conex√£o no banco de dados.

---

## Requisitos do Projeto

### Objetivo

API REST robusta que:
1. **Extrai dados** via web scraping de ~1000 livros do site books.toscrape.com
2. **Armazena** os dados em banco PostgreSQL
3. **Disponibiliza** endpoints HTTP para consulta, filtros e estat√≠sticas
4. **Prepara dados** para consumo por modelos de Machine Learning
5. **Monitora** performance com logs estruturados e m√©tricas

### Funcionalidades

| Funcionalidade | Descri√ß√£o |
|----------------|-----------|
| **Web Scraping Robusto** | Extra√ß√£o de t√≠tulo, pre√ßo, rating, disponibilidade, categoria e imagem |
| **Listagem de Livros** | Consulta paginada de todos os livros |
| **Busca e Filtros** | Busca por t√≠tulo, categoria e faixa de pre√ßo |
| **Estat√≠sticas** | M√©tricas gerais e por categoria |
| **Pipeline ML-Ready** | Features normalizadas e dataset para treinamento |
| **Autentica√ß√£o JWT** | Prote√ß√£o de rotas administrativas |
| **Monitoramento** | Logs estruturados, m√©tricas e dashboard Streamlit |
| **Documenta√ß√£o** | Swagger UI integrada em `/docs` |

### Tecnologias Utilizadas

| Tecnologia | Uso |
|------------|-----|
| **FastAPI** | Framework para APIs REST |
| **SQLAlchemy** | ORM para banco de dados |
| **Pydantic** | Valida√ß√£o de dados |
| **BeautifulSoup4** | Web scraping |
| **httpx** | Cliente HTTP ass√≠ncrono |
| **pandas** | An√°lise de dados e estat√≠sticas |
| **python-jose** | Autentica√ß√£o JWT |
| **Streamlit** | Dashboard de monitoramento |
| **PostgreSQL** | Banco de dados relacional |

---

## Arquitetura

O projeto segue uma **arquitetura modular em camadas** (Clean Architecture).

### Diagrama de Arquitetura

```mermaid
flowchart TB
    subgraph Cliente["üåê Cliente"]
        Browser["Navegador/Postman"]
        ML["Modelo ML"]
        Dashboard["Dashboard Streamlit"]
    end

    subgraph API["üì° FastAPI"]
        Router["Router v1"]
        Auth["Auth Middleware"]
        Logging["Logging Middleware"]
        
        subgraph Endpoints["Endpoints"]
            Books["/books"]
            Categories["/categories"]
            Stats["/stats"]
            MLEndpoint["/ml"]
            Health["/health"]
            Metrics["/metrics"]
            Scraping["/scraping üîí"]
        end
    end

    subgraph Services["‚öôÔ∏è Servi√ßos"]
        BookService["BookService"]
        StatsService["StatsService"]
        MLService["MLService"]
        AuthService["AuthService"]
    end

    subgraph Data["üíæ Dados"]
        Repository["Repository"]
        PG[("PostgreSQL")]
        Scraper["Web Scraper"]
        Site["books.toscrape.com"]
    end

    Browser --> Router
    ML --> MLEndpoint
    Dashboard --> Metrics

    Router --> Auth
    Auth --> Logging
    Logging --> Endpoints

    Books --> BookService
    Categories --> BookService
    Stats --> StatsService
    MLEndpoint --> MLService
    Scraping --> Scraper

    BookService --> Repository
    StatsService --> Repository
    MLService --> Repository

    Repository --> PG
    Scraper --> Site
    Scraper --> PG
```

### Estrutura de Pastas

```
8MLET-F1/
‚îú‚îÄ‚îÄ üìÅ docs/                    # Documenta√ß√£o adicional
‚îú‚îÄ‚îÄ üìÅ scripts/                 # Scripts auxiliares
‚îÇ   ‚îú‚îÄ‚îÄ run_scraper.py          # Execu√ß√£o do scraper
‚îÇ   ‚îú‚îÄ‚îÄ create_admin.py         # Cria√ß√£o de admin
‚îÇ   ‚îî‚îÄ‚îÄ dashboard.py            # Dashboard Streamlit
‚îî‚îÄ‚îÄ üìÅ src/                     # C√≥digo fonte
    ‚îú‚îÄ‚îÄ üìÅ api/                 # Camada de API (endpoints e rotas)
    ‚îÇ   ‚îî‚îÄ‚îÄ v1/endpoints/       # Endpoints versionados
    ‚îú‚îÄ‚îÄ üìÅ core/                # Configura√ß√µes centrais
    ‚îÇ   ‚îú‚îÄ‚îÄ config.py           # Settings da aplica√ß√£o
    ‚îÇ   ‚îú‚îÄ‚îÄ database.py         # Conex√£o com banco
    ‚îÇ   ‚îú‚îÄ‚îÄ middleware.py       # Logs e m√©tricas
    ‚îÇ   ‚îî‚îÄ‚îÄ logging.py          # Sistema de logs
    ‚îú‚îÄ‚îÄ üìÅ models/              # Models do banco (SQLAlchemy)
    ‚îú‚îÄ‚îÄ üìÅ schemas/             # Schemas de valida√ß√£o (Pydantic)
    ‚îú‚îÄ‚îÄ üìÅ repository/          # Camada de acesso a dados
    ‚îú‚îÄ‚îÄ üìÅ services/            # L√≥gica de neg√≥cio
    ‚îú‚îÄ‚îÄ üìÅ scraper/             # M√≥dulo de web scraping
    ‚îî‚îÄ‚îÄ main.py                 # Ponto de entrada da API
```

### Fluxo de Requisi√ß√£o

```mermaid
sequenceDiagram
    autonumber
    
    participant C as üë§ Cliente
    
    box rgb(230, 240, 255) API Layer
    participant R as üåê Router
    participant M as üõ°Ô∏è Middleware
    participant E as üìç Endpoint
    end

    box rgb(255, 250, 230) Core Logic
    participant S as üß† Service
    participant D as üíæ Repository
    end

    participant DB as üêò PostgreSQL

    C->>R: GET /api/v1/books
    activate R
    R->>M: Process Request
    activate M
    note right of M: In√≠cio Cron√¥metro
    
    M->>E: Forward Request
    activate E
    
    E->>S: get_books_paginated()
    activate S
    
    S->>D: get_all(skip, limit)
    activate D
    
    D->>DB: SELECT * FROM books
    activate DB
    DB-->>D: Return Records
    deactivate DB
    
    D-->>S: List[BookModel]
    deactivate D
    
    S-->>E: (books, total)
    deactivate S
    
    E-->>M: PaginatedResponse
    deactivate E
    
    M-->>R: Log Request + Metrics
    deactivate M
    note right of M: Fim Cron√¥metro
    
    R-->>C: JSON Response 200 OK
    deactivate R
```

---

## Hospedagem no Google Cloud Platform (GCP)

A aplica√ß√£o √© hospedada utilizando servi√ßos serverless do Google Cloud, garantindo escalabilidade autom√°tica e baixo custo. A arquitetura √© dividida em dois servi√ßos principais rodando no Cloud Run.

### Componentes do Deploy
Toda a orquestra√ß√£o √© feita pelo script deploy.sh, que automatiza os seguintes passos:

**Artifact Registry (books-repo):**
- Serve como o "Docker Hub" privado.
- Armazena as vers√µes das imagens Docker da API e do Dashboard.
- Localiza√ß√£o: us-central1.

**Cloud Build:**
- Constr√≥i as imagens Docker na nuvem (sem depender da m√°quina local).
- Usa os arquivos cloudbuild.api.yaml e cloudbuild.dashboard.yaml como instru√ß√µes.

**Cloud Run (Compute):**
- Servi√ßo 1: API (books-api):
  - Container: src/main.py (FastAPI).
  - Porta: 8000.
  - Vari√°veis: DATABASE_URL (Conex√£o Postgres), JWT_SECRET_KEY (Seguran√ßa).
- Servi√ßo 2: Dashboard (books-dashboard):
  - Container: scripts/dashboard.py (Streamlit).
  - Porta: 8501.
  - Vari√°veis: API_URL (Para consumir os dados da API).

### Fluxo de Deploy

```mermaid
graph LR
    Local[Local Machine] -->|./deploy.sh| CloudBuild[Cloud Build]
    CloudBuild -->|Build & Push| ArtifactRegistry[Artifact Registry]
    ArtifactRegistry -->|Pull Images| CloudRun[Cloud Run]
    
    subgraph GCP[Google Cloud Platform]
        CloudBuild
        ArtifactRegistry
        CloudRun
    end
    
    subgraph DeployConfig[Configura√ß√£o]
        EnvVars[Environment Variables] -.->|Inject| CloudRun
    end
```

### Deploy Autom√°tico (GCP)

Para realizar o deploy no Google Cloud Platform (Cloud Run):

1. **Configurar o script de deploy**:
   Edite o arquivo `deploy.sh` e atualize as vari√°veis `DATABASE_URL` e `JWT_SECRET_KEY`.

2. **Executar o script**:
   ```bash
   chmod +x deploy.sh
   ./deploy.sh
   ```
   
   O script ir√°:
   - Criar o reposit√≥rio no Artifact Registry (se n√£o existir)
   - Construir as imagens Docker da API e Dashboard
   - Fazer o deploy no Cloud Run configurando as vari√°veis de ambiente

**Resumo da Arquitetura:**

    Cloud Build (Constr√≥i) -> Artifact Registry (Armazena) -> Cloud Run (Executa API + Dashboard)


## Instala√ß√£o e Configura√ß√£o

### Requisitos

- Python 3.11+
- pip ou Poetry

### Passo 1: Clonar o Reposit√≥rio

```bash
git clone <url-do-repositorio>
cd MLE-P1
```

### Passo 2: Criar Ambiente Virtual

```bash
# Criar ambiente virtual
python -m venv .venv

# Ativar ambiente virtual
source .venv/bin/activate    # Linux/Mac
.venv\Scripts\activate       # Windows
```

### Passo 3: Instalar Depend√™ncias

```bash
# Via pip
pip install -r requirements.txt

# Ou via Poetry
poetry install
```

### Passo 4: Configurar Vari√°veis de Ambiente (Opcional)

Crie um arquivo `.env` na raiz do projeto:

```env
DEBUG=True
API_V1_STR=/api/v1
PROJECT_NAME="API de Livros"
DATABASE_URL=postgresql://user:pass@host:port/dbname
JWT_SECRET_KEY=sua-chave-secreta-aqui
```

---

## Execu√ß√£o

### 1. Popular o Banco de Dados (Scraping)

```bash
python scripts/run_scraper.py
```

Este comando extrai ~1000 livros e salva no banco de dados configurado.

### 2. Iniciar a API

```bash
uvicorn src.main:app --reload
```

### 3. Acessar a Documenta√ß√£o

- **Swagger UI**: [http://localhost:8000/docs](http://localhost:8000/docs)
- **ReDoc**: [http://localhost:8000/redoc](http://localhost:8000/redoc)

### 4. Executar Testes
```bash
# Executa testes unit√°rios e de integra√ß√£o
pytest
```

---

## Documenta√ß√£o das Rotas da API

Base URL: `http://localhost:8000/api/v1`

### Autentica√ß√£o (`/auth`)

| M√©todo | Rota | Descri√ß√£o |
|--------|------|-----------|
| `POST` | `/auth/login` | Autentica e retorna tokens JWT |
| `POST` | `/auth/refresh` | Renova o access token |

### Livros (`/books`)

| M√©todo | Rota | Descri√ß√£o |
|--------|------|-----------|
| `GET` | `/books/` | Lista livros com pagina√ß√£o |
| `GET` | `/books/{id}` | Detalhes de um livro espec√≠fico |
| `GET` | `/books/search` | Busca por t√≠tulo e/ou categoria |
| `GET` | `/books/top-rated` | Lista os mais bem avaliados |
| `GET` | `/books/price-range` | Filtra por faixa de pre√ßo |

### Categorias (`/categories`)

| M√©todo | Rota | Descri√ß√£o |
|--------|------|-----------|
| `GET` | `/categories/` | Lista todas as categorias |

### Estat√≠sticas (`/stats`)

| M√©todo | Rota | Descri√ß√£o |
|--------|------|-----------|
| `GET` | `/stats/overview` | Estat√≠sticas gerais (total, m√©dia, distribui√ß√£o) |
| `GET` | `/stats/categories` | Estat√≠sticas por categoria |

### Machine Learning (`/ml`)

| M√©todo | Rota | Descri√ß√£o |
|--------|------|-----------|
| `GET` | `/ml/features` | Features normalizadas para infer√™ncia |
| `GET` | `/ml/training-data` | Dataset completo para treinamento com split train/test |
| `POST` | `/ml/predictions` | Recebe e processa predi√ß√µes de modelos |

### Scraping (`/scraping`) üîí

| M√©todo | Rota | Descri√ß√£o |
|--------|------|-----------|
| `POST` | `/scraping/trigger` | Dispara scraping (requer token admin) |

### Sa√∫de (`/health`)

| M√©todo | Rota | Descri√ß√£o |
|--------|------|-----------|
| `GET` | `/health` | Verifica status da API |

### M√©tricas (`/metrics`)

| M√©todo | Rota | Descri√ß√£o |
|--------|------|-----------|
| `GET` | `/metrics/` | M√©tricas consolidadas de performance |
| `GET` | `/metrics/requests` | Requisi√ß√µes recentes com detalhes |
| `GET` | `/metrics/summary` | Resumo r√°pido de m√©tricas |

---

## Exemplos de Chamadas

### Login (Obter Token)

**Primeiro, crie o usu√°rio admin:**
```bash
python scripts/create_admin.py
```

**Request:**
```bash
curl -X POST "http://localhost:8000/api/v1/auth/login" \
  -H "Content-Type: application/json" \
  -d '{"username": "admin", "password": "admin123"}'
```

**Response:**
```json
{
  "access_token": "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...",
  "refresh_token": "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...",
  "token_type": "bearer"
}
```

---

### Refresh Token

**Request:**
```bash
curl -X POST "http://localhost:8000/api/v1/auth/refresh" \
  -H "Content-Type: application/json" \
  -d '{"refresh_token": "<REFRESH_TOKEN>"}'
```

**Response:**
```json
{
  "access_token": "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...",
  "refresh_token": "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...",
  "token_type": "bearer"
}
```

---

### Disparar Scraping (Rota Protegida)

**Request:**
```bash
curl -X POST "http://localhost:8000/api/v1/scraping/trigger" \
  -H "Authorization: Bearer <ACCESS_TOKEN>"
```

**Response:**
```json
{
  "message": "Scraping iniciado em background",
  "triggered_by": "admin"
}
```

---

### Listar Livros (Paginado)

**Request:**
```bash
curl -X GET "http://localhost:8000/api/v1/books/?page=1&limit=10"
```

**Response:**
```json
{
  "total": 1000,
  "page": 1,
  "limit": 10,
  "items": [
    {
      "id": 1,
      "title": "A Light in the Attic",
      "price": 51.77,
      "rating": 3,
      "availability": true,
      "category": "Poetry",
      "image_url": "https://..."
    }
  ]
}
```

---

### Buscar por T√≠tulo

**Request:**
```bash
curl -X GET "http://localhost:8000/api/v1/books/search?title=Python"
```

**Response:**
```json
[
  {
    "id": 123,
    "title": "Learning Python",
    "price": 39.99,
    "rating": 5,
    "availability": true,
    "category": "Programming",
    "image_url": "https://..."
  }
]
```

---

### Buscar por Categoria

**Request:**
```bash
curl -X GET "http://localhost:8000/api/v1/books/search?category=Fiction"
```

**Response:**
```json
[
  {
    "id": 45,
    "title": "The Great Gatsby",
    "price": 22.50,
    "rating": 4,
    "availability": true,
    "category": "Fiction",
    "image_url": "https://..."
  }
]
```

---

### Obter Detalhes de um Livro

**Request:**
```bash
curl -X GET "http://localhost:8000/api/v1/books/1"
```

**Response:**
```json
{
  "id": 1,
  "title": "A Light in the Attic",
  "price": 51.77,
  "rating": 3,
  "availability": true,
  "category": "Poetry",
  "image_url": "https://..."
}
```

---

### Livros Mais Bem Avaliados

**Request:**
```bash
curl -X GET "http://localhost:8000/api/v1/books/top-rated?limit=5"
```

**Response:**
```json
[
  {
    "id": 15,
    "title": "Best Book Ever",
    "price": 29.99,
    "rating": 5,
    "availability": true,
    "category": "Classics",
    "image_url": "https://..."
  }
]
```

---

### Filtrar por Faixa de Pre√ßo

**Request:**
```bash
curl -X GET "http://localhost:8000/api/v1/books/price-range?min_price=10&max_price=30"
```

**Response:**
```json
[
  {
    "id": 22,
    "title": "Affordable Book",
    "price": 19.99,
    "rating": 4,
    "availability": true,
    "category": "Non-Fiction",
    "image_url": "https://..."
  }
]
```

---

### Listar Categorias

**Request:**
```bash
curl -X GET "http://localhost:8000/api/v1/categories/"
```

**Response:**
```json
[
  "Fiction",
  "Poetry",
  "Fantasy",
  "Romance",
  "Science Fiction",
  "Mystery"
]
```

---

### Estat√≠sticas Gerais

**Request:**
```bash
curl -X GET "http://localhost:8000/api/v1/stats/overview"
```

**Response:**
```json
{
  "total_books": 1000,
  "avg_price": 35.07,
  "min_price": 10.00,
  "max_price": 59.99,
  "rating_distribution": {
    "1": 50,
    "2": 100,
    "3": 300,
    "4": 350,
    "5": 200
  },
  "top_categories": [
    {"name": "Fiction", "count": 150},
    {"name": "Romance", "count": 120}
  ]
}
```

---

### Estat√≠sticas por Categoria

**Request:**
```bash
curl -X GET "http://localhost:8000/api/v1/stats/categories"
```

**Response:**
```json
[
  {
    "category": "Fiction",
    "total_books": 150,
    "avg_price": 32.50,
    "avg_rating": 3.8
  },
  {
    "category": "Poetry",
    "total_books": 80,
    "avg_price": 28.00,
    "avg_rating": 4.2
  }
]
```

---

### Features para Machine Learning

**Request:**
```bash
curl -X GET "http://localhost:8000/api/v1/ml/features"
```

**Response:**
```json
[
  {
    "id": 1,
    "price_norm": 0.85,
    "rating": 3,
    "category_code": 12,
    "availability": true
  }
]
```

---

### Dataset para Treinamento

**Request:**
```bash
curl -X GET "http://localhost:8000/api/v1/ml/training-data?test_size=0.2"
```

**Response:**
```json
{
  "total_samples": 1000,
  "feature_names": ["price_norm", "rating", "category_code", "availability_int"],
  "target_name": "rating",
  "train": {
    "X": [[0.83, 3.0, 0.0, 1.0], ...],
    "y": [3, 1, 4, ...],
    "size": 800
  },
  "test": {
    "X": [[0.45, 2.0, 1.0, 1.0], ...],
    "y": [2, 5, 3, ...],
    "size": 200
  },
  "metadata": {
    "category_mapping": {"Fiction": 0, "Poetry": 1, ...},
    "price_min": 10.0,
    "price_max": 59.99,
    "test_size": 0.2
  }
}
```

---

### Enviar Predi√ß√µes de Modelos

**Request:**
```bash
curl -X POST "http://localhost:8000/api/v1/ml/predictions" \
  -H "Content-Type: application/json" \
  -d '{"predictions": [{"book_id": 1, "predicted_rating": 4.5, "confidence": 0.92}]}'
```

**Response:**
```json
{
  "processed": 1,
  "rejected": 0,
  "predictions": [
    {"book_id": 1, "predicted_rating": 4.5, "confidence": 0.92}
  ],
  "errors": null
}
```

---

### Verificar Sa√∫de da API

**Request:**
```bash
curl -X GET "http://localhost:8000/api/v1/health"
```

**Response:**
```json
{
  "status": "saud√°vel",
  "version": "1.0.0",
  "timestamp": "2026-01-01T19:00:00.000000"
}
```

---

## Dashboard de Monitoramento

O projeto inclui um dashboard Streamlit para visualiza√ß√£o de m√©tricas em tempo real.

O Dashboard √© uma aplica√ß√£o separada constru√≠da com a biblioteca Streamlit (scripts/dashboard.py), que consome dados de performance expostos pela pr√≥pria API.

**Arquitetura do Dashboard**

Coleta de Dados (LoggingMiddleware):
- Existe um interceptador (middleware) em src/core/middleware.py.
- Ele cronometra cada requisi√ß√£o (tempo de in√≠cio vs. fim).
- Armazena m√©tricas (status code, tempo de resposta, endpoint) em uma mem√≥ria tempor√°ria (MetricsStore).

Exposi√ß√£o dos Dados (/api/v1/metrics):
- A API possui endpoints espec√≠ficos para exportar esses dados em JSON.
- GET /metrics/: Retorna contadores gerais (Total de Requests, Erros, Uptime).
- GET /metrics/requests: Retorna a lista detalhada das √∫ltimas requisi√ß√µes.

Visualiza√ß√£o (Streamlit):
- O script dashboard.py faz chamadas HTTP peri√≥dicas para esses endpoints.
- Usa Pandas para transformar os JSONs em DataFrames.
- Renderiza gr√°ficos e tabelas interativas.

**Executar o Dashboard:**
```bash
# Primeiro, inicie a API
uvicorn src.main:app --reload

# Em outro terminal, inicie o dashboard
streamlit run scripts/dashboard.py
```

O dashboard exibe:
- Total de requisi√ß√µes e taxa de erro
- Gr√°ficos por endpoint, status code e m√©todo HTTP
- Tabela de requisi√ß√µes recentes com filtros

---
## Documenta√ß√£o Adicional

Para mais detalhes sobre a implementa√ß√£o, consulte:

- [Arquitetura](docs/architecture.md) - Diagrama de arquitetura
- [Guia da API](docs/api_guide.md) - Refer√™ncia r√°pida dos endpoints
